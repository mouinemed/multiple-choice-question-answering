{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedmouine/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, BertForMultipleChoice\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"BertForMultiChoiceCoveo\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = BertForMultipleChoice.from_pretrained(model_checkpoint)\n",
    "\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('multiple_choice/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Evaluation Using Pre-trained BERT Model**\n",
    "\n",
    "The first step of using the pre-trained BERT model to predict the missing word choice aims to establish a baseline performance for the NLP challenge. By evaluating the accuracy of the pre-trained model on the task, we can assess its out-of-the-box suitability and identify potential areas for improvement. This initial assessment will serve as a reference point for gauging the effectiveness of subsequent fine-tuning efforts with the provided data, allowing us to measure the extent of performance gains achieved through model adaptation to the specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df rows: 1000it [26:24,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.60%\n",
      "CPU times: user 26min, sys: 4min 19s, total: 30min 19s\n",
      "Wall time: 26min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to predict the label for a given row\n",
    "def predict_label(text, choice1, choice2, choice3, choice4, choice5, choice6):\n",
    "    \"\"\"\n",
    "    This function takes a passage of text containing a [BLANK], and six candidate choices (choice1 to choice6) as input.\n",
    "    It uses the tokenizer to encode the text and each choice, creating a PyTorch tensor of the encoded data.\n",
    "    The tokenizer converts the text and choices into numerical representations that can be fed into the pre-trained BERT model.\n",
    "    The return_tensors=\"pt\" option specifies that the output should be PyTorch tensors.\n",
    "    The padding, truncation, and max_length arguments ensure that the input data is properly formatted for the BERT model.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): A passage of text containing a [BLANK].\n",
    "        choice1 to choice6 (str): Candidate words that could replace the [BLANK].\n",
    "\n",
    "    Returns:\n",
    "        predicted_label (int): The index of the most probable choice among the candidates.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer([text]*6, [choice1, choice2, choice3, choice4, choice5, choice6], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})\n",
    "    logits = outputs.logits\n",
    "    predicted_label = logits.argmax(dim=1).item()\n",
    "    return predicted_label\n",
    "\n",
    "# Sample DataFrame \n",
    "\n",
    "df=df_train[:1000]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), desc='df rows'):\n",
    "    predicted_label = predict_label(row['text'], str(row['choice1']), str(row['choice2']), str(row['choice3']), str(row['choice4']), str(row['choice5']), str(row['choice6']))\n",
    "    true_label = row['label']\n",
    "    \n",
    "    if predicted_label == df.columns.get_loc(true_label) - 2:  # Convert 'choice1', 'choice2', etc. to integer index\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_rows\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### will repeat same experience without NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df rows: 1000it [17:58,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 24.10%\n",
      "CPU times: user 17min 22s, sys: 2min 56s, total: 20min 19s\n",
      "Wall time: 17min 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to predict the label for a given row\n",
    "def predict_label(text, choice1, choice2, choice3, choice4):\n",
    "    \"\"\"\n",
    "    This function takes a passage of text containing a [BLANK], and six candidate choices (choice1 to choice6) as input.\n",
    "    It uses the tokenizer to encode the text and each choice, creating a PyTorch tensor of the encoded data.\n",
    "    The tokenizer converts the text and choices into numerical representations that can be fed into the pre-trained BERT model.\n",
    "    The return_tensors=\"pt\" option specifies that the output should be PyTorch tensors.\n",
    "    The padding, truncation, and max_length arguments ensure that the input data is properly formatted for the BERT model.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): A passage of text containing a [BLANK].\n",
    "        choice1 to choice6 (str): Candidate words that could replace the [BLANK].\n",
    "\n",
    "    Returns:\n",
    "        predicted_label (int): The index of the most probable choice among the candidates.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer([text]*4, [choice1, choice2, choice3, choice4], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})\n",
    "    logits = outputs.logits\n",
    "    predicted_label = logits.argmax(dim=1).item()\n",
    "    return predicted_label\n",
    "\n",
    "# Sample DataFrame \n",
    "\n",
    "df=df_train[:1000]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), desc='df rows'):\n",
    "    predicted_label = predict_label(row['text'], str(row['choice1']), str(row['choice2']), str(row['choice3']), str(row['choice4']))\n",
    "    true_label = row['label']\n",
    "    \n",
    "    if predicted_label == df.columns.get_loc(true_label) - 2:  # Convert 'choice1', 'choice2', etc. to integer index\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_rows\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Initial Evaluation Using Pre-trained BERT Model\n",
    "\n",
    "In the context of the NLP challenge provided by Coveo, the initial evaluation using the pre-trained BERT model resulted in an accuracy of approximately 14.60% to 24.10%. This accuracy is quite low and indicates that the pre-trained BERT model, without any fine-tuning on the specific task, is not sufficiently effective in predicting the correct choice for the missing word.\n",
    "\n",
    "## The Need for Fine-tuning a Model with Coveo Data\n",
    "\n",
    "The need for the next step, which is fine-tuning a model with Coveo data, arises from the following observations:\n",
    "\n",
    "1. **Low Accuracy:** The initial evaluation shows that the pre-trained BERT model's out-of-the-box performance is not adequate for this specific NLP task. Fine-tuning the model can lead to significant improvements in accuracy and overall performance.\n",
    "\n",
    "2. **Domain-specific Data:** The challenge data is sourced from multiple public webpages, including Coveo's public pages. By fine-tuning the model with this specific data, we can make the model more familiar with Coveo's language and writing style, making it more suitable for the task at hand.\n",
    "\n",
    "3. **Task-specific Context:** Fine-tuning allows the model to learn task-specific patterns and dependencies that are crucial for correctly predicting the missing word. By incorporating the specific context and semantics of the challenge data, the model can better understand and reason about the relationships between the passage of text and the candidate word choices.\n",
    "\n",
    "4. **Reducing Biases and Improving Generalization:** Fine-tuning can help reduce biases present in the pre-trained model and improve its generalization to the specific task requirements. By fine-tuning with task-specific data, we can potentially enhance the model's ability to handle a diverse range of examples and make more accurate predictions.\n",
    "\n",
    "5. **Performance Benchmarking:** After fine-tuning the model, we can re-evaluate its accuracy and other metrics. This will allow us to measure the progress and effectiveness of the fine-tuning process, comparing it against the initial evaluation to gauge the extent of improvement achieved.\n",
    "\n",
    "Overall, fine-tuning a model with Coveo data is essential to create a more task-specific, accurate, and reliable model that can effectively predict the missing word choice in the given sentences. This iterative process of fine-tuning, evaluation, and refinement is crucial to building a successful model that meets the requirements and objectives of the NLP challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a multiple choice task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In the next sessions, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model to a multiple choice task, which is the task of selecting the most plausible inputs in a given selection. The dataset used here is COVEO data but we can adapt the pre-processing to any other multiple choice dataset we like. \n",
    "\n",
    "This code is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a mutiple choice head. Depending on our model and the GPU we are using, we might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = df_train[3000:15000] #continue for the next 12k input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-bd390af26803>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['label'].map(label_to_id)\n"
     ]
    }
   ],
   "source": [
    "# map label to IDs \n",
    "label_to_id = {'choice1': 0,'choice2': 1,'choice3': 2, 'choice4': 3, 'choice5': 4, 'choice6': 5,}\n",
    "df['label'] = df['label'].map(label_to_id)\n",
    "\n",
    "# Due to lack of time, I am only considering the first 4 choices because they do not contain any 'NaN'.\n",
    "df = df[~df['label'].isin([4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Split the DataFrame into train (80%), validation (10%), and test (10%) sets\n",
    "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the split DataFrames into datasets\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "valid_data = Dataset.from_pandas(valid_data)\n",
    "test_data = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice1 has 0 NaN values.\n",
      "choice2 has 0 NaN values.\n",
      "choice3 has 0 NaN values.\n",
      "choice4 has 0 NaN values.\n",
      "choice5 has 111553 NaN values.\n",
      "choice6 has 114463 NaN values.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    column_name = f'choice{i}'\n",
    "    has_nan = df_train[column_name].isna().sum()\n",
    "    print(f\"{column_name} has {has_nan} NaN values.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "choices = ['choice1', 'choice2', 'choice3', 'choice4']\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    texts = [[text] * 4 for text in examples[\"text\"]] \n",
    "    # Extract choices\n",
    "    choices_list = []\n",
    "    for i in range(len(examples['choice1'])):\n",
    "        choices = [examples[f'choice{j}'][i] for j in range(1, 5)]\n",
    "        choices_list.append(choices)\n",
    "\n",
    "    # Flatten everything\n",
    "    texts = sum(texts, [])\n",
    "    choices_list = sum(choices_list, [])\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(texts, choices_list, truncation=True)\n",
    "    # Un-flatten\n",
    "    tokenized_examples = {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists of lists for each key: a list of all examples (here 5), then a list of all choices (4) and a list of input IDs (length varying here since we did not apply any padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9352834d1b4b45f383748c5d9a4e3bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af980c4ea624bb1b35813a341a5efd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb39e57201434595d9e082cbb50b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing to the train, validation, and test datasets\n",
    "encoded_train_data = train_data.map(preprocess_function, batched=True)\n",
    "encoded_valid_data = valid_data.map(preprocess_function, batched=True)\n",
    "encoded_test_data = test_data.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'text', 'choice1', 'choice2', 'choice3', 'choice4', 'choice5', 'choice6', 'label', '__index_level_0__'],\n",
       "    num_rows: 1197\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'text', 'choice1', 'choice2', 'choice3', 'choice4', 'choice5', 'choice6', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1198\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training arguments\n",
    "args = TrainingArguments(\n",
    "    \"bert-base-uncased-finetuned-coveo\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,  # Disable this to avoid pushing to the Hub\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_train_data,\n",
    "    eval_dataset=encoded_valid_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedmouine/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1797' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1797 68:06:57 < 7:27:05, 0.01 it/s, Epoch 2.70/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.373910</td>\n",
       "      <td>0.870510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.471039</td>\n",
       "      <td>0.883876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "output_dir = \"bert-base-uncased-finetuned-coveo\"\n",
    "args = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"epoch\",        # Save the model checkpoint after each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_accuracy\",  # Metric to monitor for saving the best model\n",
    "    greater_is_better=True,       # Set to True if higher metric value is better\n",
    "    save_total_limit=2,           # Limit the total number of saved checkpoints to 2\n",
    "    \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer with the EarlyStoppingCallback\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_train_data,\n",
    "    eval_dataset=encoded_valid_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# The best model checkpoint(s) will be saved at \"bert-base-uncased-finetuned-coveo/checkpoint-xx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir='BertForMultiChoiceCoveo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
